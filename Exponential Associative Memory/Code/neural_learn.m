%==========================================================================
%********************FUNCTION: clustered_neural_learn**********************
%==========================================================================

%--------------------------------INPUTS------------------------------------
% N: The number of pattern nodes in the graph
% K: The dimension of the subspace of the pattern nodes
% L: The number of clusters if we have multiple levels (L = 1 for single level)
% const_learn: Number of constraints which must be learned during the learning phase
% index_in: The index of the simulation setup among various random scenarios
% alpha0: The step size in the learning algorithm
% beta0: The sparsity penalty coefficient in the learning algorithm
% theta0: The sparsity threshold in the learning algorithm
%--------------------------------------------------------------------------

%--------------------------------OUTPUTS-----------------------------------
% None
%--------------------------------------------------------------------------


%--------------------------FUNCTION DESCRIPTION----------------------------
% This function implements the idea mentioned in our journal paper to learn 
% a sparse vector W which is orthogonal to a set of given patterns. 

% The code starts by reading patterns at random and adjusting the weights 
% accordingly such that in the end we have Wx = 0, for all data vectors x 
% in the training set. The learning approach which is very similar (almost 
% identical to) the one proposed in the paper "The subspace learning 
% algorithm as a formalism for pattern recognition and neural networks" 
% plus a penalty term to make the results sparse. 

% The code stops once a maximum number of iterations is passed or
% convergence is reached. Once finished, the learnt vectors are added to
% the end of the appropriatefile. 
%--------------------------------------------------------------------------


%==========================================================================
%==========================================================================

%function neural_learn_ITW_journal(N,K,const_learn,train_set_index,alpha0,beta0,theta0,mca_flag)

%=============================INITIALIZATION===============================

if (~exist('initialization_done_by_master','var'))    % If already not initialized by the GUI..

    %------------------------Simulation Variables--------------------------
    K = 150;                            % Number of message bits
    N = 300;                            % Number of pattern neurons in the network
    Q = 8;                              % Number of quantization levels
    const_to_learn = 150;               % Number of contraints to learn over the patterns
    index_in = 1;                       % Index of the random graph in the considered ensemble (for real_dataset_flag =1)    
    %----------------------------------------------------------------------
    
    %-----------------------Other Initializations--------------------------
    max_y_thr = .001;
    theta0 = 0.02;                      % The initial sparisty threshold
    alpha0 = 0.9;                       % The initial learning rate
    learn_itr_max = 500;                % The maximum number of performed learning iterations
    beta0 = 0.8;                        % The sparsity penalty
    
    addpath(genpath('../Common_Library'));                          % Include the library of common functions in the search path
    
    a=clock;                                % Initialize the seed for random number generation with the clock value.
    RandStream.setGlobalStream(RandStream('mt19937ar','seed',sum(100*a)));

    %---Determine if the Dataset is Synthetic or Contains Real Patterns----
    real_dataset_flag = 1;              % If 0, this flag tells the code to use the dataset generated by the file "neural_initialization.m". If 1, it will read the dataset from the file specified by the user
        
    if real_dataset_flag
        db_file_name = ['CIFAR_10_train_gray_scale_class_4.mat'];
        db_file_folder = ['./Database/CIFAR_10'];    % Make sure that there is no "/" or "\" at the end of the folder name
        db_name_in = 'CIFAR_10_Gray_DB_class_4';    % The name of the matrix that contains the patterns as its rows
    end
    %---------------------------------------------------------------------
    
    W_tot = [];
end

%----------------------Create a Directory if Necessary---------------------
if real_dataset_flag == 0
    mkdir(['../Learn_Results'],['N_',num2str(N),'_K_',num2str(K)]);        % Create a specific folder for the current N and K
    learn_folder = ['../Learn_Results/N_',num2str(N),'_K_',num2str(K)];
else
    inds =  strfind(db_file_folder,'/');
    temp_folder = db_file_folder(inds(length(inds))+1:length(db_file_folder));
    mkdir(['../Learn_Results/',temp_folder]);
    learn_folder = ['../Learn_Results/',temp_folder];
end
%--------------------------------------------------------------------------

%==========================================================================

%=======================PREPROCESS LEARNING DATASET========================

%--------------Load the Random Synthetic Training Dataset------------------
if real_dataset_flag == 0
    fid = fopen(['../Initialization_Files/N_',num2str(N),'_K_',num2str(K),'/neural_journal_train_set_N_',num2str(N),'_K_',num2str(K),'_index_',num2str(index_in),'.mat'], 'r');                        % The path towards the dataset
    if (fid > -1 )
        fclose(fid);
        load(['../Initialization_Files/N_',num2str(N),'_K_',num2str(K),'/neural_journal_train_set_N_',num2str(N),'_K_',num2str(K),'_index_',num2str(index_in),'.mat']);
    else    
        error('I can not find the learning dataset!');    
    end
    
    
    %...........................Create the Dataset.........................
    dataset_learn = zeros(length(mu_list),N);
    [C,~] = size(dataset_learn);
    for mu = 1:length(mu_list)
        
        %~~~~~~~~~~~~~~~~~~~~~Pick a Pattern at Random~~~~~~~~~~~~~~~~~~~~~
        index = 1+floor((length(mu_list)-1)*rand);
        temp = dec2bin(mu_list(index),K);                      
        message = zeros(1,K);           % Generate the message from the index                                             
        for j = 1:K                    
            message(j) = (temp(j) - 48);
        end            
        dataset_learn(mu,:) = message*G;
        %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    end
    %......................................................................
    
%--------------------------------------------------------------------------    

%-----------------Load the Real-Valued Training Dataset--------------------
else
    cd('..')
    fid = fopen([db_file_folder,'/Preprocessed/','Preprocessed_',db_file_name], 'r');
    if (fid > -1) 
        fclose(fid);        
        load([db_file_folder,'/Preprocessed/','Preprocessed_',db_file_name]);                                    
        [C,N] = size(dataset_learn);            
    else    
        error('I can not find the learning dataset!');    
    end
     cd('./Code')
end
%--------------------------------------------------------------------------



%==========================================================================

%=============================LEARNING PHASE===============================
for itr = 1: const_to_learn

    %------------------Randomly Initialize the Weight Matrix---------------
    W = random_vector(N,4*round(log(N)));
    W = W'; 
    W = W/norm(W);
    %----------------------------------------------------------------------
    
    %--------------------Adjust Simulation Parameters----------------------
    theta = theta0;       
    cost_per_itr = [];
    theta_itr = 1;    
    %----------------------------------------------------------------------

    %--------------------------Main Learning Loop------------------------------                
    for learn_itr = 1:learn_itr_max         % Repeat the steps below several times in order to enhance the quality of the learning phase        
        temp_cost = 0;
        max_y = 0;
        
        alph0 = max(50*alpha0/(50+log(learn_itr)),0.0005);
        continue_flag = 0;
        for mu = 1:C
            
            %------------------Pick a Pattern at Random--------------------
            index = 1+floor((C-1)*rand);     
            x = dataset_learn(index,:);                                                    
            %--------------------------------------------------------------
            
            %--------------------Update the Weight Vector------------------
            y = x*W;                 % Calculate the projection of x on the vector W(j,:).
            
            if (norm(x)>.001)                
                alph = alph0/norm(x)^2;
            end
            if (norm(W) > .001)
                %W = soft_threshold(W - alph*y*( x' - (y*W/(norm(W))^2)),theta);
                % W = soft_threshold(W - alph*y*( x' - (y*W/(norm(W))^2)),theta);%-beta0*W.*(1-(tanh(100*W.^2)).^2);
                
                W = soft_threshold(W - alph*y*(x'*(2*norm(W)^2-1) - y*W),theta);
                % W = soft_threshold(W - alph*y*(x'-y*W)+ alph*(1-norm(W)^2)*W,theta);
                % W = soft_threshold(W - alph*y*( x' - (y*W/(norm(W))^2)),theta);%-beta0*W.*(1-(tanh(100*W(j,:).^2)).^2);
            end
            
            %--------------------------------------------------------------
        
            
            %-----------------Check for Numerical Errors-------------------
            if (sum(isnan(W))>0)    
                display('Error Nan');        
                break;                                    
            end
            
            if (norm(W) < .001)                
                display('Error zero!');
            end
            %--------------------------------------------------------------

            %-----------------Update the Simulation Costs------------------
            temp_cost = temp_cost + y^2;
            if (abs(y)> max_y)
                max_y = abs(y);
            end                                        
            %--------------------------------------------------------------                                                            
            
        end 
         
        theta_itr = theta_itr + 1;
        theta = theta0/theta_itr;
        
        cost_per_itr = [cost_per_itr,norm(dataset_learn*W)/C];
        %----------------------Check Convergence---------------------------
        if (norm(dataset_learn*W)/C < max_y_thr)        
            break;
        end
        %------------------------------------------------------------------      
        
         
         
    end  
    
    display('Another learn_itr has passed!');
    %======================================================================


    %========================SAVE THE RESULTS==============================
    %if (max_y <max_y_thr) 
    W = soft_threshold(W,0.025);
    a = dataset_learn*W;
    if (sum(abs(a)<0.025)>0.9*C)
        W_tot = [W_tot;W'];
        fid = fopen([learn_folder,'/W_alpha_',num2str(alpha0),'_theta_',num2str(theta0),'_index_',num2str(index_in),'.txt'], 'a');
        fprintf(fid, '%f \t',W');
        fprintf(fid, '\n');
        fclose(fid);

        fid = fopen([learn_folder,'/Learn_itr_alpha_',num2str(alpha0),'_theta_',num2str(theta0),'_index_',num2str(index_in),'.txt'], 'a');
        fprintf(fid, '%d \t',learn_itr);
        fprintf(fid, '\n');
        fclose(fid);

        fid = fopen([learn_folder,'/Learn_cost_alpha_',num2str(alpha0),'_theta_',num2str(theta0),'_index_',num2str(index_in),'.txt'], 'a');
        fprintf(fid, '%f \t',cost_per_itr);
        fprintf(fid, '\n');
        fclose(fid);
    end
    %==========================================================================
end

