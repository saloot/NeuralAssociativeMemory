clc
%clear all;


%=============================INITIALIZATION===============================

N = 2^9-1;                     % N is the number of neurons in network.
k = 8;                       % k is the number of message bits.
M = (N-3)/2;

theta = 0;                   % theta is the neural threshold. 
max_itr = 2;                 % This is the maximum number of iteration for the simulation.
deg_ave = 3;                 % This is the average degree of the LDGM code.error_count = 0;             % This counts the number of errors over iterations.
err_bits = 5;                % This is the number of erroneous bits
beta0 = 1000;                   % beta determines the degree of stochasticity of the Hopfield network
error_count = zeros(1,21);             % This tracks the number of errors.
bit_error_count = zeros(1,21);
ensemble_flag = 0;           % 0 means random patterns are used. 1 is for codewords. 2 is for a given set of patterns
convergence_flag = 1;        % 0 means you repeat for max_itr times. 1 means you wait untill convergence or do 1000 iteration

W = zeros(N,N);              % W is the weight matrix.
Y = zeros(M,N);              % Y is the matrix of patterns;
x = zeros(1,N);              % x is the vector of states;
%==========================================================================


if (ensemble_flag ~= 2)
    no_codes_generated = 100;% This is the number different random codes generated by the progra
else
    no_codes_generated = 1;
end
%====================PICK PATTERNS AND THE INITIAL STATE===================



for out_loop = 1:no_codes_generated
    
    if (ensemble_flag == 0)
        Y = randint(M,N);
        Y = -2*Y+ones(M,N);    
    elseif (ensemble_flag == 1)
        M = 2^k;                    
        %----------------------Build the Generator Matrix------------------
        G = zeros(k,N);
        for j = 1:N
            for i = 1:k
                p = rand;
                if (p<deg_ave/k)
                    G(i,j) = 1;
                end
            end
        end
        %------------------------------------------------------------------

        %------------------Construct the Codewords-------------------------       
        for i = 0:M-1  
            temp = dec2bin(i,k);
            message = zeros(1,k);
            for j = 1:k
                message(j) = temp(j) - 48;
            end   
            Y(i+1,:) = mod(message*G,2);    
        end
        %------------------------------------------------------------------
        Y = -2*Y + ones(M,N);
        YY = Y';

    else
        m = 7;
        N = 2^m -1;
        alph = gf([2],m);
        %run gold
       
        Y = G(1:M,:);

        %-------------------Calculate Correlation of Patterns--------------
        %[corr,pd,ax] = correlation_patterns(Y);
        %------------------------------------------------------------------
    end
    

    
    
   


    %======================================================================


    %=========================DETERMINE THE OMEGA'S========================
    Omega = zeros(1,M);
    alph = gf([2],7);

    %Omega(M) = 1;
    for mu = 0:M-1
        aa = gftrace(alph^(-mu),7);      
        
        if ((-1)^(double(aa.x))==1)
            Omega(mu+1) = 1;
        else
            Omega(mu+1) = -1;
        end
    end
   
    %======================================================================



    %=======================DETERMINE THE WEIGHTS==========================
    
    Lambda = diag(100*ones(1,M));
    W = Y'*Lambda *inv(Y*Y')*Y;

error_itr = 0;
for err_bits = 0:1:20                % This is the number of erroneous bits
    error_itr = error_itr + 1;
%=========INTIALIZE THE NETWORK WITH A PATTERN AND ITERATE=============
error_ind = [];
for kk = 1:500
    
    %------------------------------Add Noise---------------------------
    nois = ones(1,N);
    %         for nn = 1:N
    %             pp = rand;
    %             if (pp<nois_prob)
    %                 nois(nn) = -1;
    %             end
    %         end
    pp = 1+round((N-1)*rand(1,err_bits));
    for h = 1:err_bits
        nois(pp(h)) = -1;       
    end
    %------------------------------------------------------------------
    
    
    p = round((M-1)*rand)+1;
    x = Y(p,:).*nois; % Initialize the network with the first pattern.
    x0 = Y(p,:);
    x_last = zeros(1,N);
    
    %--------Update Neurons' States Synchronously and Iteratively------
    itr = 0;
    exit_flag = 0;
    while (exit_flag == 0)
        itr = itr + 1;
        
        %------------------Calculate the Weighted Input Sum-----------------
        x_temp = W*x';
        x_last = x;
%         beta = max(5,beta0/(10*itr));
        beta = beta0;
        for iii = 1:N
            pp = rand;
            if (pp > 1/(exp(-2*beta*x_temp(iii))))
                x(iii) = -1;
            else
                x(iii) = 1;
            end
        end
        
        % Remain in the loop until convergence or until we exceed 1000
        % iterations:
        if ( (norm(x-x_last) == 0)||(itr == 1000))
            exit_flag = 1;
        end
        
        
        
        
        
        
    end
    %------------------------------------------------------------------    
    if (norm(x - x0)~=0)
        error_count(error_itr) = error_count(error_itr) + 1;
        error_ind = [error_ind p];
        bit_error_count(error_itr) =  bit_error_count(error_itr)+ (norm(x-x0)^2)/4;       
    end
    
    if (mod(kk,10000) == 0)
        display(['Iteration:',num2str(kk), ' Error count so far: ',num2str(error_count(error_itr))]);
    end
    
end
%==========================================================================

end
display(' ');
display(['Pattern Error Rate ',num2str(error_count/(kk*out_loop))]);
display(['Bit Error Rate ',num2str(bit_error_count/kk/N/out_loop)]);
out_loop
end